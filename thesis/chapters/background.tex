\chapter{Background}
\label{chap:background}

\section{Basics of Fully Homomorphic Encryption}
\gls{he} makes it possible to operate on data without knowing it.
One can distinguish three flavors of it, Partial-, Semi- and \gls{fhe}.

% Fully Homomorphic Encryption (FHE):
\begin{itemize}
  \item Brakerski/Fan-Vercauteren (BFV) scheme for integer arithmetic
  \item Brakerski-Gentry-Vaikuntanathan (BGV) scheme for integer arithmetic
  \item Cheon-Kim-Kim-Song (CKKS) scheme for real-number arithmetic
  \item Ducas-Micciancio (FHEW) and Chillotti-Gama-Georgieva-Izabachene (TFHE) schemes for Boolean circuit evaluation
\end{itemize}

\subsection{Packing}
FFT in CKKS!
Polynom -> Vektor mit einer FFT
Vektor -> Polynom mit einer IFFT

\subsection{HE using RSA}
With unpadded RSA, some arithmetic can be performed on the ciphertext -
looking at the encrypted ciphertext $\mathcal{E}(m_1) = (m_1)^r \mod n$
of the message $m_1$ and $m_2$ respectively, the following holds:
\begin{align}
  \mathcal{E}(m_1) \cdot \mathcal{E}(m_2)
   & = (m_1)^r (m_2)^r \mod n     \\
   & = (m_1 m_2)^r \mod n         \\
   & = \mathcal{E}(m_1 \cdot m_2)
\end{align}

The encryption therefore partially fulfills the properties of ring homomorphism, which in general is defined as follows:

\begin{definition}{Ring Homomorphism}{ring-homomorphism} ~
  Given two rings $(R, +, \cdot)$ and $(S, \oplus, \otimes)$, we call a mapping $\varphi: R \rightarrow S$
  a ring homomorphism when it satisfies the following conditions:
  $$\forall a, b \in R: \varphi(a + b) = \varphi(a) \oplus \varphi(b) \wedge \varphi(a \cdot b) =
    \varphi(a) \otimes \varphi(b)$$
\end{definition}

\subsection{Learning with Errors (LWE)}

\subsection{Ring-LWE}
Learning with Errors on Rings (RLWE)

how to get from LWE to RLWE

\subsection{The BFV scheme}
\subsection{The CKKS scheme}
The CKKS scheme allows us to perform approximate arithmetic on floating point numbers.

\pagebreak
\section{Machine Learning}
Undoubtedly one of the most prevalent concepts in todays computing world, \gls{ml} has shaped
how computers think and how we interact with them significantly.
As Shafi \name{Goldwasser} puts it, 'Machine Learning is somewhere in the intersection of Artificial Intelligence,
Statistics and Theoretical Computer Science' \parencite{goldwasserTalk2018}.

Within the scope of this thesis, the basics of neural networks and associated learning methods shall be covered,
limited to the category of supervised learning problems (as opposed to unsupervised learning problems).
Supervised learning refers to the machine \textit{training} an algorithm to match some input data (features)
with corresponding output data (targets), often related to pattern recognition.
The trained algorithm can then be utilised to match fresh input data with a prediction of the targets.

A popular subset of applications to \gls{ml} are classification problems, predominantly image classification,
which was not as easily possible before without a human eye due to the lack of computing power.
Classification problems can be formulated quickly, the goal is to computationally categorize input data
(for instance, images) into a predefined set of classes (for instance, cats and dogs).
The primary concept behind \acrlong{ml} is not at all new, linear regression
was already employed by \name{GauÃŸ} and \name{Legendre} in the early 19\textsuperscript{th} century;
the term 'Neural Network' was first used by \name{McCulloch} and \name{Pitts} in 1943.
Much media attention was earned in the 2000-2010 decade when larger image classification problems
became feasible with the increasing computational power of modern computers,
up until the advent of Deep Learning \parencite{bishop-pattern-recognition-and-ml}.

\subsection{Linear Regression}
Given an input vector $\vec{x} \in \R^n$, the goal of linear regression is to predict the value of a target $t \in \R$,
according to some model $M$.

\subsection{Gradient Descent}
\subsection{Multi-Layered Neural Networks}

\begin{theorem}{Universal Approximation Theorem}
  If the neural network has at least one hidden layer, proper nonlinear activation functions and enough
  data and hidden units, it can approximate any continuous function $y(x, w): \R^n \mapsto \R$
  arbitrarily well on a compact domain
  \parencite{1989-HornikMultilayerFN}.
\end{theorem}

Matrix -> Activation Function
\begin{itemize}
  \item Matrix Multiplication (Dense Layer)
  \item Convolutional Layer
  \item Sigmoid Activation
  \item Max Pooling
\end{itemize}

\subsection{The Backpropagation Algorithm}

\section{Post-Quantum Security}
\subsection{Shor's Algorithm}

\section{Demo}
In this chapter, we provide some usage examples for
glossaries and acronym lists with \texttt{glossaries} (\autoref{sec:gloss}),
bibliography and citations with \texttt{biblatex} (\autoref{sec:bib}), and more.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/taylor-relu.png}
  \caption{Comparison of the Relu activation function vs. its Taylor expansion}
\end{figure}

\section{Notation and Acronyms}
\label{sec:gloss}
Symbols and acronyms are defined in the preamble, after loading the \texttt{glossaries} package, and used as follows.

In this chapter, we introduce the necessary background on the \gls{aes}.
We denote binary exclusive-or by \gls{xor}.

\label{sec:bib}
This is an example of how to specify and cite
a book \cite{AESbook},
a journal article \cite{bstjShannon49}.
\Gls{aes} is a block cipher defined by \textcite{AESbook}.
