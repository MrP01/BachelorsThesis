\chapter{Background}
\label{chap:background}
\vspace{-1cm}

The discussion of the \gls{he} schemes following in \cref{chap:homomorphic-encryption} requires some mathematical background that will be introduced here, aiming for a consistent overview rather than full completeness.
The last two sections \ref{sec:machine-learning} and \ref{sec:post-quantum-sec} introduce some background on \glsdesc{ml} and provide an outlook on Quantum Computation and why it affects cryptography today.

% \section{Spicy Math for \gls{he}}
\paragraph{Notational Conventions:}
Let $\N$ denote the natural numbers without $0$.
For a probability distribution $\chi$ over a set $R$, let sampling a value $x \in R$ from the probability distribution be denoted by $x \leftarrow \chi$.
For $a \in \R$ a real number, denote rounding down (floor) $a$ by $\lfloor a \rfloor \in \Z$, rounding up (ceil) by $\lceil a \rceil \in \Z$ and rounding to the nearest integer by $\lfloor a \rceil \in \Z$.
Let $[a]_q := a \mod q$ denote the positive remainder when dividing $a$ by $q$.

\section{Polynomial Rings and Modular Arithmetic}
As the algebraic structure underlying almost every single symbol following in the next chapters, we recall the definition of a ring:

\begin{definition}{Ring}{ring}
  A tuple $(R, +, \cdot)$ consisting of a set $R$, an addition operation $+$ and a multiplication operation $\cdot$
  is referred to as a ring, given that it satisfies the following \textit{ring axioms}:
  \begin{itemize}
    \item Addition is closed: $a + b \in R \quad\forall a, b \in R$.
    \item Addition is commutative: $a + b = b + a \quad\forall a, b \in R$.
    \item Addition is associative: $(a + b) + c = a + (b + c) \quad\forall a, b, c \in R$.
    \item There exists an element $0 \in R$ such that $a + 0 = a \quad\forall a \in R$.
    \item An additive inverse $-a$ of each element $a$ in $R$ exists, such that $a + (-a) = 0$.
    \item Multiplication is associative: $(a \cdot b) \cdot c = a \cdot (b \cdot c) \quad\forall a, b, c \in R$.
    \item Multiplication is closed: $a \cdot b \in R \quad\forall a, b \in R$.
    \item There exists an element $1 \in R$, referred to as the identity element, or multiplicative identity of $R$,
          such that $a \cdot 1 = a \quad\forall a \in R$.
    \item Multiplication $\cdot$ is distributive w.r.t. addition $+$, \\
          i.e. $a \cdot (b+c) = (a \cdot b) + (a \cdot c) \quad\forall a, b, c \in R$ from the left and \\
          i.e. $(b+c) \cdot a = (b \cdot a) + (c \cdot a) \quad\forall a, b, c \in R$ from the right.
  \end{itemize}
  If multiplication is additionally commutative, we refer to the ring as \textit{commutative}:
  \begin{itemize}
    \item Multiplication is commutative: $a \cdot b = b \cdot a \quad\forall a, b \in R$.
  \end{itemize}
\end{definition}

Acting as the logical extension of a group, a ring can be considered the intermediary step towards a field (which also defines subtraction and division).
Recall that the first 5 properties can be summarised as $(R, +)$ forming an Abelian group.
An example of a ring would be the integers themselves, or the integers modulo $q$: $\Z / q \Z$, sometimes also denoted as $\Z_q$.

Given two groups $(G, +)$ and a subgroup $(N, +)$, we can construct another group $G / N$ as follows, referred to as a quotient group or factor group:

\begin{definition}{Quotient Group / Ring}{quotient-group}
  A quotient group $(G / N, +)$ (pronounced '$G$ mod $N$') over the original group $G$ and a normal subgroup $N$ of $G$
  with a standard element operation $+$ can be defined using the left cosets
  $$g+N := \{g+n \,|\, n \in N\} \subseteq G$$ of $N$ in $G$.
  The corresponding set $G / N$ is defined as
  $$G / N := \{g + N \,|\, g \in G\}$$
  whereas the standard operation $+: G/N \times G/N \mapsto G/N$
  can be extended from the original group $G$ as follows ($g, h \in G$):
  $$(g+N) + (h+N) := (g+h)N$$
\end{definition}

The quotient set $G / N$ can therefore be identified as the set of all possible left cosets $g + N$ that in union reconstruct the original group $G$.

As a highly relevant structure to cryptography and a great example of a quotient group, we would like to consider the ring of integers modulo a given modulus $q \in \N$.

\begin{lemma}{Ring of Integers Modulo $q$: $\Z / q \Z$}{integers-modulo-q}
  Using equivalence classes $\overline{x}_q$ modulo $q$ referred to as congruence classes,
  define the commutative quotient ring of integers modulo $q$ as $(\Z / q \Z, +, \cdot)$ with
  two operations $+$ and $\cdot$ and
  $$\Z / q \Z = \{\overline{x}_q \,|\, x \in \Z, 0 \leq x < q\}$$
  where $q \Z = \{q x \,|\, x \in \Z\} \triangleleft \Z$ (where $\triangleleft$ refers to the left being a subgroup of the right) denotes the $q$\textsuperscript{th} multiplicative coset\footnote{
    from the left and from the right, therefore $q \Z$ is called a normal subgroup of $\Z$
  } of the integers and
  $$\overline{x}_q = \{y \equiv x \mod q \,|\, y \in \Z\}$$
  is the set of all multiples of $q$ with remainder $x$.
  Note that many operations that resulting groups, rings or fields are commonly equipped with,
  such as addition or multiplication, propagate to an equivalent definition in the ring of integers modulo $q$
  by considering their result as a congruence class instead of it, which in turn is again an element of $\Z / q \Z$.
\end{lemma}

This ring is of specific importance in discrete mathematics and can be regarded as a formalisation of modular arithmetic, much of which we will require at a later point in this chapter.

As a first step towards the first central result, \cref{corollary:bounded-polynomials-mod-q}, we formally introduce polynomial rings and how to carry out addition and multiplication between them.

\begin{definition}{Polynomial Ring over $\Z$}{poly-ring}
  On the set of all complex-valued polynomials with integer coefficients (a function space)
  $$\Z[X] = \big\{p: \C \mapsto \C \,, p(x) = \sum_{k=0}^\infty a_k x^k, a_k \in \Z \;\forall k \geq 0\big\},$$
  we can define a commutative ring $(\Z[X], +, \cdot)$ equipped with the standard addition $+$ and multiplication $\cdot$ operations (as an extension over the field $\C$) of polynomials.
\end{definition}

To further elaborate on the polynomial ring operations:
\begin{itemize}
  \item In their coefficient representations $\vec{p} = (p_j)_{j\in\N} = (p_0, p_1, p_2, ...)$ (which are sequences) and $\vec{q} = (q_j)_{j\in\N} = (q_0, q_1, q_2, ...)$, an addition of two polynomials $p, q \in \Z[X]$ is equivalent to the element-wise addition of their coefficient sequences
        \begin{align*}
          (p + q)(X) & = \sum_{k=0}^\infty p_k X^k + \sum_{k=0}^\infty q_k X^k = \sum_{k=0}^\infty (p_k + q_k) X^k \\
                     & = \langle (\vec{p} + \vec{q}), \{X^0, X^1, X^2, ...\}^T \rangle
        \end{align*}
        which indeed satisfies the additive ring axioms (cf. \cref{def:ring}) due to the existing structure of the underlying field $\C$.
  \item The multiplication operation can be defined using a discrete convolution of the coefficient vectors
        $$r(X) = (p \cdot q)(X) = (\sum_{k=0}^\infty p_k X^k) \cdot (\sum_{l=0}^\infty q_l X^l)
          = \sum_{k=0}^\infty \sum_{l=0}^\infty p_k q_l X^{k+l}
          = \sum_{k=0}^\infty r_k X^k$$
        with the arising coefficients $(r_k)_{k\in\N}$ determined by the discrete convolution
        $$r_k = \sum_{l=0}^k p_l q_{k-l} \;\Leftrightarrow\; \vec{r} = \vec{p} * \vec{q}$$
        in this context also referred to as the \name{Cauchy}-product. Therefore,
        $$(p \cdot q)(X) = \langle (\vec{p} * \vec{q}), \{X^0, X^1, X^2, ...\}^T \rangle.$$
        Again, this generally applicable approach satisfies the multiplicative ring axioms and even satisfies commutativity due to the existing structure of the underlying field $\C$ and the symmetry of convolutions.
\end{itemize}

Where $\langle \cdot, \cdot \rangle$ denotes the dot (scalar) product between two vectors.

Polynomials with degree $\geq 1$ over the complex numbers can always be factorised using their roots due to the fundamental theorem of algebra.
Polynomials over the integers however, cannot always be factorised further, yielding the definition of an irreducible polynomial.

\begin{definition}{Irreducible Polynomials}{irreducible-polys}
  A polynomial is called irreducible \gls{iff} it cannot be written as a product of other polynomials \textsl{while staying in the same coefficient space}.
\end{definition}

\subsection{Cyclotomic Polynomials}
Due to their interesting structure and efficient computability, in the schemes introduced in the following chapter, certain polynomials (\cref{corollary:bounded-polynomials-mod-q}) are chosen as representations of plaintexts and ciphertexts.
An important concept is that of cyclotomic ('circle-cutting') polynomials, which we will discuss in a bit more detail here.

An important polynomial is $$p: \C \mapsto \C, \; p(x) = x^n - 1 \,.$$
Its roots, found by solving $p(\xi) = 0$ for $\xi$, yielding $\xi^n = 1 \leftrightarrow \xi_k = \sqrt[n]{1}$ are referred to as the $n$\textsuperscript{th} roots of unity, of which there are multiple for each $n \in \N$.

\begin{lemma}{The $n$\textsuperscript{th} roots of unity}{nth-roots-of-unity}
  For some integer $n \in \N$, the $n$ complex roots $\xi_1, \xi_2, ..., \xi_n \in \C$ of unity can be found as $$\xi_k = e^{2\pi i \frac{k}{n}} \quad k \in \{1, 2, ..., n\}$$ with $i$ being the imaginary unit.
  Confer \cref{fig:nth-roots-of-unity}.
  Using \name{Euler}'s identity, their real and imaginary components can be explicitly found as $\xi_k = \cos(2\pi \frac{k}{n}) + i \sin(2\pi \frac{k}{n})$.

  An $n$\textsuperscript{th} root of unity $y$ is referred to as \textit{primitive}, \gls{iff} there exists no $m < n$ for which that root $y$ is also an $m$\textsuperscript{th} root of unity, i.e. $y^m \neq 1$.
  An equivalent indicator of a primitive root is $\gcd(m, n) = 1$, referring to the greatest common divisor between $m$ and $n$ which is $1$ \gls{iff} they are mutually prime.
\end{lemma}
Due to the fact that for any $k, l \in \Z$, their product $\xi_k \cdot x_l$ is also a root of unity, and $\xi_{k+jn} = \xi_k \; \forall j \in \Z$, they clearly comprise a cyclic Abelian group over the complex numbers $\C$ under multiplication with (for instance) the first root $\xi_1 = e^{2\pi i \frac{1}{n}}$ as its generator.

\begin{figure}
  \centering
  \inputtikz{figures/nth-roots-of-unity}
  \caption[The 5th roots of unity]{The 5\textsuperscript{th} roots of unity visualised on the complex plane. Obviously, they all lie on the unit circle $|z| = 1$, motivating the name of cyclotomic, 'circle-cutting', polynomials, whose roots cut the unit circle into multiple sectors.}
  \label{fig:nth-roots-of-unity}
\end{figure}

\begin{definition}{Cyclotomic Polynomial}{cyclotomic-poly}
  Given the $n$\textsuperscript{th} roots of unity $\{\xi_k\}$, we can define the $n$\textsuperscript{th}
  cyclotomic polynomial $\Phi_n \in \Z[X]$ as the product over all primitive roots of unity
  $$\Phi_n(x) = \prod_{\stackrel{k=1}{\xi_k \mathrm{primitive}}}^{n} (x - \xi_k) \,.$$
  It is unique for each given $n \in \N$.
\end{definition}
The number of primitive roots of unity is given by $\varphi(n)$, denoting \name{Euler}'s totient function which counts the natural numbers $m$ less than $n$ who do not share a common divisor $\neq 1$, i.e. $\gcd(m, n) = 1$.
$\varphi(n)$ therefore also counts the number of primitive roots of unity for $n$, consequently also yielding the degree of the $n$\textsuperscript{th} cyclotomic polynomial.

An important aspect of cyclotomic polynomials is that they are irreducible over their coefficient space, the integers $\Z$.
\begin{remark}{Irreducibility of Cyclotomic Polynomials}{cyclotomic-irreducibility}
  Cyclotomic polynomials are always irreducible.
\end{remark}
This enables us to \textit{uniquely} define a quotient ring with cyclotomic polynomials as moduli, later.
In theory, there are multiple equivalent definitions of said ring, but by convention we choose the cyclotomic polynomial because it cannot be simplified further.
The proof for \cref{remark:cyclotomic-irreducibility} is quite cumbersome, but can be found in \cite{2002-serge-algebra}.

\begin{theorem}{$2^k$\textsuperscript{th} Cyclotomic Polynomial}{power-of-2-cyclo-poly}
  The $N$\textsuperscript{th} cyclotomic polynomial, where $M = 2N = 2^k$ ($k \in \N$) is a power of $2$, can be identified as
  $$\Phi_{M}(x) = x^{N} + 1\,.$$
  Its degree is $N$, consistent with $\varphi(2^k) = 2^{k-1} \; \forall k \in \N$.
\end{theorem}
Find a short but illustrative proof of \cref{thm:power-of-2-cyclo-poly} in \cref{proof:power-of-2-cyclo-poly}.

\begin{definition}{Ring of Polynomials of highest degree $N-1$}{bounded-polynomials}
  For a power-of-2 $N$, one can construct the quotient ring $(R, +, \cdot)$ as
  $$R = \Z[X] / (X^N + 1)$$
  where $(X^N + 1)$ denotes the set of all polynomial multiples of the polynomial $p \in \Z[X], p(x) = x^N + 1$, so
  $$(X^N+1) = \{q: \C \mapsto \C,\; q(x) = r(x) \cdot (x^N+1) \;|\; r \in \Z[X]\} \,.$$
  The elements of $R$ are then polynomials with integer coefficients of maximum degree $N-1$.
\end{definition}

If $N$ is a power of $2$, according to \cref{thm:power-of-2-cyclo-poly}, $$R = \Z[X] / \Phi_d(X) = \Z[X] / (X^N + 1)$$ is the set of integer-coefficient polynomials reduced modulo $\Phi_d(X)$, the $d$\textsuperscript{th} cyclotomic polynomial with $N = \varphi(d) = \frac{d}{2}$.
Since every cyclotomic polynomial is irreducible, this is a unique representation of $R$ without any possible further simplifications.
Therefore, in the following we will focus on power-of-2 cyclotomic polynomials, which turn out to be even more useful when defining FFT-optimized operations on them.

As promised above, we will require \cref{lemma:integers-modulo-q} for the fundamental structure underlying the \gls{he} schemes described in the next chapter, defining ourselves a ring with coefficients in said quotient ring $\Z/q\Z$.

\begin{corollary}{Polynomial Ring modulo $q$}{bounded-polynomials-mod-q}
  Further modifying $R = \Z[X] / (X^N+1)$ for $N$ a power of $2$ to only take coefficients mod $q$, we obtain two equivalent definitions for the same ring:
  $$R_q = R/qR = (\Z/q\Z)[X] / (X^N+1)$$
  which contains polynomials with integer coefficients modulo $q$ of degree $N-1$.
  Explicitly stated, the set can be written as:
  $$R/qR = \{p: \C \mapsto \C,\; p(x) = \sum_{k=0}^{N-1} a_k x^k \;|\; a_k \in \Z/q\Z\}$$
\end{corollary}

This bounded polynomial ring is central to understanding objects in the next chapter and \cref{corollary:bounded-polynomials-mod-q} can be regarded as the central result of this section.

\pagebreak
\section{Lattice Cryptography}
\label{subsec:lattice-crypto}
Lattice-based cryptography takes a different approach to encryption than classical factorisation or the discrete logarithm problem, as it is based on different hardness assumptions, namely ones on \hyperref[def:lattice]{lattice} problems.
The goal of any mathematical encryption scheme is to leave a potential attacker with a computationally hard, at best infeasible, problem to solve when attempting to decrypt messages without a secret key.
This section will start with three basic problems, SVP, GapSVP and SIS and move on to \gls{lwe} and \gls{rlwe}.
To illustrate the connection of these problems to lattices, we take a closer look at them before considering further details of LWE.
Most notably, lattice problems are conjectured to be secure against quantum computers \parencite{2018-lattice-problems}.

\newcommand{\lat}{\mathcal{L}}
\begin{definition}{Lattice}{lattice}
  A lattice $(\lat, +, \cdot)$ is a vector field over the integers $(\Z, +, \cdot)$, defined using a set of $n$ basis vectors $\vec{b_1}, \vec{b_2}, ..., \vec{b_n} \in \R^n$, that can be introduced as a set
  $$\lat = \bigg\{\sum_{i=1}^n c_i \vec{b}_i \,\bigg|\, c \in \Z\bigg\} \subseteq \R^n$$
  equipped with at least vector addition $+: \lat \times \lat \mapsto \lat$ and scalar multiplication $\cdot: \Z \times \lat \mapsto \lat$.
  As an extension of $\R^n$, the Euclidean norm $||\cdot||$ is also defined and the standard Euclidean metric $d: \lat \times \lat \mapsto \R$, yielding a metric space $(\lat, d)$, can be obtained by the norm of a vector difference, denoted $||(\cdot) - (\cdot)||$.
\end{definition}

Lattices are a common concept appearing in many areas of mathematics and physics, related to their effective representation as data structures and also geometric intuition (cf. \cref{fig:lattice}).
Its \textit{minimum distance} $\lambda_{min}$ is defined as the smallest Euclidean distance between two points $\vec{p_1}, \vec{p_2} \in \lat$
$$\lambda_{min} = \min_{\vec{p_1}, \vec{p_2} \in \lat} d(\vec{p_1}, \vec{p_2}) =
  \min_{\vec{p_1}, \vec{p_2} \in \lat} ||\vec{p_1} - \vec{p_2}|| \,,$$
which can be equivalently thought of as the minimal length of any non-zero vector in the lattice $\lat$, because of $\vec{0}$ always being an element of the lattice which can be chosen as $\vec{p_1}$ and the translational symmetry between fundamental lattice volumes (or regions).

The three problems frequently showing up in cryptography are stated below, each taking a different approach in their own interesting way.

\begin{definition}{Shortest Vector Problem (SVP)}{svp}
  Given a lattice $\lat$ constructed from $n$ basis vectors, find the shortest non-zero lattice vector $\vec{x} \in \lat \backslash \{\vec{0}\}$, i.e. find $\vec{x}$ such that $||\vec{x}|| = \lambda_{min}$ \parencite{2016-decade-of-lattice}.
\end{definition}

Based on SVP, one can construct GapSVP, an approximative version with advantages for usage in practical problems.

\begin{definition}{Decisional Approximate SVP (GapSVP)}{gap-svp}
  Given a lattice $\lat$ and some pre-defined function $\gamma: \N \mapsto \R$ depending on the lattice dimension $n$ (constant for a given $\lat$) with $\gamma(n) \geq 1$, the decisional approximate shortest vector problem is distinguishing between $\lambda_{min} \leq 1$ and $\lambda_{min} > \gamma(n)$.
  For other cases, it is up to the algorithm what to return.
\end{definition}

\begin{definition}{Short Integer Solution (SIS) Problem}{sis}
  For $m$ given vectors $(\vec{a}_i)_{0 < i \leq m} \in (\Z / q\Z)^n$ that comprise the columns of a matrix
  $A \in (\Z / q\Z)^{n \times n}$ and an upper bound $\beta$, find
  a solution vector $\vec{z} \in \Z^n \backslash \{\vec{0}\}$ such that
  $$A \vec{z} = \vec{0} \quad \mathrm{with} \quad ||\vec{z}|| \leq \beta\,.$$
\end{definition}

Note that without the last requirement $||\vec{z}|| \leq \beta$, the \gls{sis} problem can be easily solved through Gaussian elimination or similar algorithms, however they rarely yield a short (or \textit{the} shortest) solution.
It can be shown that solving \gls{sis} is at least as hard as solving \gls{gapsvp} with appropriate parameters \parencite{1996-hard-lattice-problems}.

Using the above problems, multiple cryptographic primitives can be constructed due to the proven hardness that also propagates to quantum computers.
Examples include collision resistant hash functions, signatures, pseudorandom functions or even Regev's public-key cryptosystem that is based on \gls{lwe}, which is reduced to the other lattice problems \parencite{2016-decade-of-lattice}.

\begin{figure}
  \centering
  \inputtikz{figures/lattice}
  \caption[Illustration of a standard lattice]{Illustration of a standard lattice $\lat$ over the integers $\Z$ with two basis vectors $\vec{b}_1$ and $\vec{b}_2$ (cf. \cref{def:lattice}).
    The shortest vector problem in this case is solved by $\vec{x} = 0 \vec{b}_1 \pm 1 \vec{b}_2$.}
  \label{fig:lattice}
\end{figure}

\subsection{Learning with Errors (LWE)}
\label{subsec:lwe}
Next, we would like to consider \gls{lwe}, a computing problem that is believed to be sufficiently hard to be used in cryptography and, most notably, is not yet solvable in linear time by a quantum algorithm (cf. \cref{sec:post-quantum-sec}), like any other cryptographic lattice problem so far.
Its hardness assumptions are related to GapSVP and were first formally proven by \citeauthor{2005-lwe-original}, for which he received the 2018 Gödel price.

\begin{definition}{LWE-Distribution $A_{\vec{s}, \chi_{error}}$}{lwe-dist}
  Given a prime $p \in \N$ and $n \in \N$, we choose some secret $\vec{s} \in (\Z / p \Z)^n$.
  In order to sample a value from the LWE distribution $A_{\vec{s}, \chi_{error}}$:
  \begin{itemize}
    \item Draw a random vector $a \in (\Z/p\Z)^n$ from the multivariate uniform distribution
          with its domain in the integers up to $p$.
    \item Given another probability distribution $\chi_{error}$ over the integers modulo $p$,
          sample a scalar 'error term' $\mu \in \Z / p \Z$ from it, often also referred to as noise.
    \item Set $b = \vec{s} \cdot \vec{a} + \mu$, with $\cdot$ denoting the standard vector product.
    \item Output the pair $(\vec{a}, b) \in (\Z / p \Z)^n \times (\Z / p \Z)$.
  \end{itemize}
\end{definition}

The general approach useful to cryptography is to sample an element from the LWE-distribution and construct two problems out of it, \textit{search}-LWE and \textit{decision}-LWE.

\begin{definition}{LWE-Problem - Search Version}{lwe-search-problem}
  Given $m$ independent samples $(\vec{a}_i, b_i)_{0 < i \leq m}$ from $A_{\vec{s}, \chi_{error}}$, find the secret $\vec{s}$.
\end{definition}
\begin{definition}{LWE-Problem - Decision Version}{lwe-decision-problem}
  Given $m$ samples $(\vec{a}_i, b_i)_{0 < i \leq m}$, distinguish (with non-negligible advantage) whether they were drawn from $A_{\vec{s}, \chi_{error}}$ or from the uniform distribution $u$ over $(\Z / p \Z)^n \times (\Z / p \Z)$.
\end{definition}

In their above definitions, \citeauthor{2005-lwe-original} showed that the two problems are equivalent.

\begin{theorem}{Hardness of LWE}{lwe-hardness}
  If there exists an efficient algorithm that solves either search-LWE or decision-LWE then there exists an efficient algorithm that approximates the decision version of the shortest vector problem (GapSVP) in the worst case \parencite{2010-lwe-survey}.
\end{theorem}

He also provided a construction of a public-key cryptosystem based on them, i.e. an asymmetric cryptographic system for at least two parties that includes a public and corresponding private key.

Public-key cryptosystems are fundamentally different from symmetric systems, which only require one single key for encryption and decryption at the same time, known by all involved parties.
Often times, public-key schemes (rather slow) are used to exchange keys for subsequent symmetric encryption (rather fast) of large plaintexts, for instance in the \gls{tls} protocol \parencite{rfc8446}.

\subsection{Learning with Errors on Rings (RLWE)}
Very similar to \cref{def:lwe-dist}, the Ring-LWE distribution is derived as follows \parencite{2010-rlwe-original}:

\begin{corollary}{RLWE-Distribution $B_{\vec{s}, \chi_{error}}$}{rlwe-dist}
  Given a quotient \hyperref[def:ring]{ring} $(R/qR, +, \cdot)$, we choose some secret $s \in R/qR$.
  In order to sample a value from the RLWE distribution $B_{s, \chi_{error}}$:
  \begin{itemize}
    \item Uniformly randomly draw an element $a \in R/qR$
    \item Given another probability distribution $\chi_{error}$ over the ring elements,
          sample an 'error term' $\mu \in R/qR$ from it, also referred to as noise.
    \item Set $b = s \cdot a + \mu$, with $\cdot$ denoting the ring multiplication operation.
    \item Output the pair $(a, b) \in R/qR \times R/qR$.
  \end{itemize}
\end{corollary}

In the exact same manner as in \cref{subsec:lwe}, the search and decision problems can be constructed.

\begin{corollary}{RLWE-Search Problem}{search-rlwe}
  Given $m$ independent samples $(a_i, b_i)_{0 < i \leq m}$ from $B_{s, \chi_{error}}$, find the secret $s$.
\end{corollary}
\begin{corollary}{RLWE-Decision Problem}{decision-rlwe}
  Given $m$ samples $(a_i, b_i)_{0 < i \leq m}$, distinguish (with non-negligible advantage)
  whether they were drawn from $B_{s, \chi_{error}}$ or from the uniform distribution
  $u$ over $R/qR \times R/qR$.
\end{corollary}

The main advantage of RLWE over LWE is that is conceptually similar and yet simple to formalise over an arbitrarily chosen \hyperref[def:ring]{ring} $(R, +, \cdot)$ which allows for a vast amount of applications and interesting constructions.

In \gls{lwe}-based cryptosystems, the public key consists of $m$ LWE-distribution (\cref{def:lwe-dist}) samples of $A_{\vec{s},\chi_{error}}$ hiding the secret $\vec{s}$.
An attacker would thereby need to solve the LWE-Problem (\cref{def:lwe-search-problem}) in order to retrieve the secret key from the public key, which is highly undesirable for a solid cryptosystem of course, but also hardly feasible with well-chosen parameters, assuming the hardness of the LWE problem (cf. \cref{thm:lwe-hardness}).
For RLWE, the public key consists of $m$ RLWE-distribution samples (\cref{corollary:rlwe-dist}) which are usually smaller since they are only comprised of elements in $R/qR$.
The size of the secret key in LWE therefore scales with $n \cdot m$, the public key with $nm + n$, while in RLWE the secret key is only a single element in $R/qR$ and the public key only scales with $2m$.
Keys are usually smaller in RLWE, depending on the choices of $p$, $q$, $n$ and $m$.

Due to their similarity, RLWE samples can even be translated into equivalent LWE samples.
In the case above, a straightforward way is to encode the polynomial coefficients of the RLWE public and secret key into a matrix $A \in (\Z/p\Z)^{m \times n}$, vector $\vec{b} \in (\Z/p\Z)^m$ and vector $\vec{s} \in (\Z/p\Z)^n$ to arrive at the corresponding LWE keys.
A similar approach may be chosen for the relinearisation (and possibly, Galois) keys.

This translation can be used to infer security requirements from LWE (a well-studied problem) over to RLWE to find secure parameters of the cryptosystem.

\pagebreak
\section{Machine Learning}
\label{sec:machine-learning}
Undoubtedly one of the most prevalent concepts in todays computing world, \gls{ml} has shaped how computers think and how we interact with them significantly.
As Shafi \name{Goldwasser} puts it, \textquote[\cite{goldwasserTalk2018}]{\textit{Machine Learning is somewhere in the intersection of Artificial Intelligence, Statistics and Theoretical Computer Science}}.

Within the scope of this thesis, the basics of neural networks and associated learning methods shall be covered, limited to the category of supervised learning problems (as opposed to unsupervised learning problems).
Supervised learning refers to the machine \textit{training} an algorithm to match some input data (features) with corresponding output data (targets), often related to pattern recognition.
The trained algorithm can then be utilised to match fresh input data with a prediction of the targets.

A popular subset of applications to \gls{ml} are classification problems, predominantly image classification, which was not as easily possible before without a human eye due to the lack of computing power.
Classification problems can be formulated quickly, the goal is to computationally categorize input data (for instance, images) into a predefined set of classes (for instance, cats and dogs).
The primary concept behind \acrlong{ml} is not at all new, linear regression was already employed by \name{Gauß} and \name{Legendre} in the early 19\textsuperscript{th} century; the term 'Neural Network' was coined by \name{McCulloch} and \name{Pitts} in 1943.
Much media attention was earned in the 2000-2010 decade when larger image classification problems became feasible with the increasing computational power of modern computers, up until the advent of Deep Learning \parencite{bishop-pattern-recognition-and-ml}.
% Keep in mind however, that Machine Learning in this form is nothing more than glorified statistical regression.

\begin{definition}{Linear Regression}{linear-regression}
  Given an input vector $\vec{x} \in \R^n$, the goal of linear regression is to predict the value of a target $t \in \R$, according to some linear model $M$.
\end{definition}

To illustrate the concept, we will focus on a simple learning method, namely that of gradient descent.
In supervised learning problems, this technique first requires us to introduce a loss (error) function $L: \R^n \mapsto \R$, usually \gls{mse}, which has comparably nice convergence properties due to its parabolic shape:
$$L(\vec{w}) = \frac{1}{2} \sum_{i=1}^N (t_i - \vec{w}^T \phi(\vec{x_i})) = \frac{1}{2} (\vec{t} - \Phi \vec{w})^T (\vec{t} - \Phi \vec{w})$$
where $\vec{w} \in \R^n$ represents the weights and $\Phi \in \R^{N \times (n+1)}$ is an auxiliary matrix introduced for compact notation, consisting of basis functions $\phi: \R^N \mapsto \R^N$ applied to the inputs $\vec{x_i}$, referred to as the design matrix.
This approach allows for a great deal of flexibility when working with more complicated datasets, simply choosing a suitable basis often reduces the problem to a perfectly linear one, easing the fitting process.
When $L(\vec{w^*}) = 0$, this means we have found the perfect weights, since our predictions exactly match the targets (labels) $t_i$.
This is not always possible, so we aim for the minimum error between predictions and targets.
In other words, our goal is to find
$$\vec{w^*} = \underset{\vec{w} \in \R^n}{\text{argmin}}\, L(\vec{w})$$
given a dataset $\{\vec{x}_i, t_i\}$.

\subsection{Gradient Descent}
\begin{figure}[H]
  \centering
  \inputtikz{figures/gradient-descent}
  \caption[Illustration of Gradient Descent]{
    An illustration of Gradient Descent on a given loss function $L(w_x, w_y)$ in parameter space $(w_x, w_y)$, adapted from \cite{gradient-descent-plot}.
    At each iteration, gradient descent advances in the opposite direction of the gradient $-\nabla L$ to approach a local minimum.
  }
  \label{fig:gradient-descent}
\end{figure}

A very common method to find such a minimum is \gls{gd}, a straightforward iterative technique to find nearby minima, given a starting position $\vec{w_0}$ in 'parameter space'.
In its simplest form, \gls{gd} simply evaluates the \textit{gradient} of the loss function $L$ at the starting point $\vec{w_0}$, yielding a direction in parameter space in which the loss will increase the most at this given point.
Therefore, we advance in the opposite direction given by $-\nabla L$, by a distance $\eta$.
In the next iteration, our subsequent guess for the local minimum is then given by $$\vec{w_{i+1}} = \vec{w_i} - \eta \nabla L$$
which we choose as the next starting point to repeat the same process as can be seen in \cref{fig:gradient-descent}.
The iteration finishes when $||\nabla L|| = 0$ (and hopefully the Hessian at $\vec{w_i}$ is positive definite) or when a recurring loop in the iteration sequence is detected, or when the loss variation $|L(\vec{w_{i+1}}) - L(\vec{w_i})|$ deceeds a given threshold \parencite{bishop-pattern-recognition-and-ml}.

Note that without modification, \gls{gd} is not a reliable method to find global minima, only local ones.
An effective optimisation would be mixing \gls{gd} with Monte-Carlo Markov Chain methods, traversing through parameter space given some probability distribution, and performing \gls{gd} subsequently at multiple locations, thereby escaping the local minima's wells to possibly reach a global minimum.
Another useful optimisation is to make the distance $\eta$ dependent on the iteration step, causing larger jumps in the beginning and smaller ones towards the end - effectively preventing ineffective jump loops around the minimum without approaching the minimum any further.

One of the biggest advantages of \glsdesc{gd} is its versatility, given any differentiable loss function, no matter how complicated, at least some progress can be made with \gls{gd}.
If the loss function has a simpler form (if it can be written as a quadratic form for instance), and to make up for numerical problems and potentially slow convergence, \gls{gd} can be replaced by more sophisticated methods such as Conjugate Gradient (with convergence guarantees within a certain boundary) or by adding in momentum to the distance travelled in each \gls{gd} iteration \parencite{bishop-pattern-recognition-and-ml}.

\subsection{Multi-Layered Neural Networks}
As the relations behind data become more and more complicated, the demand for more sophisticated modelling methods increases.
Frank \name{Rosenblatt} first implemented the \textit{Perceptron} function invented by \name{McCulloch} and \name{Pitts}, an object that closely resembles the neural network structures still in use today.
The perceptron is a function $\cryptop{Perceptron}_{\vec{w}, b}: \R^n \mapsto {0, 1}$ defined as follows:
$$\cryptop{Perceptron}_{\vec{w}, b}(\vec{x})={\begin{cases}
    1 & {\text{if }}\ \vec{w} \cdot \vec{x} +b>0, \\
    0 & {\text{otherwise}}
  \end{cases}}$$

It only has binary output, on which it decides by performing a dot product between the weights vector $\vec{w} \in \R^n$ and the input, before adding a bias $b \in \R$ to it.
When given a binary target dataset $\{\vec{x_i}, t_i\}$, the goal of the training process is to determine the weights $\vec{w}$ and bias $b$ such that $\cryptop{Perceptron}_{\vec{w}, b}(\vec{x_i}) = t_i$ for as many samples as possible.

\name{McCulloch} and \name{Pitts} further employed multiple consecutively connected perceptrons to form a larger 'network', each one referred to as a layer.
Neural networks used today are very similar in their structure (cf. \cref{fig:neural-network}).
A neural network usually consists of multiple layers of potentially different types, commonly used ones include \textit{Dense ('Fully Connected') Layers} (essentially, matrix multiplication + bias), \textit{Convolutional Layers} (given a kernel, they perform a discrete multivariate convolution over the dataset) and mixtures of non-linear, differentiable, activation functions, max-pooling, etc. in between.
Each Dense Layer is usually followed by an activation function such as $$\cryptop{relu}(\vec{x}) := \max(\vec{x}, 0) \,,$$ $$\cryptop{softmax}(\vec{x}) := \frac{e^{\vec{x}}}{\sum_{i=1}^{n} e^{x_i}} \,,$$ $\cryptop{tanh}(\vec{x})$ or $\cryptop{sigmoid}(\vec{x})$.
They usually play the role of keeping the output bounded and/or sorting for 'activated' values
\parencite{bishop-pattern-recognition-and-ml}.

\begin{figure}[H]
  \centering
  \inputtikz{figures/neural-network}
  \caption[Neural Network illustration resembling the one used in our demonstrator]{A simple neural network resembling the structure we use in our demonstrator, the input (a 784 entry vector) is forwarded to the second layer using $\vec{h} = \cryptop{relu}(M_1 \vec{x} + \vec{b_1})$, resulting in a vector of 128 entries, and finally forwarded to the output layer with $\vec{y} = \cryptop{softmax}(M_2 \vec{h} + \vec{b_2})$. Each of the 10 outputs in $\vec{y}$ corresponds to a 'probability' associated with each digit from 0 to 9.}
  \label{fig:neural-network}
\end{figure}

The training process is more complicated in the case of a layered network, especially with non-linear activation functions in between.
Yet, the basic principle behind the training process stays the same: Evaluating the loss function $L$ (depending on all weights, biases, convolutional kernels and other parameters in the network) and finding the direction in parameter space in which the loss shrinks, formalised by the layer-wise gradient (for which we require the activation functions to be differentiable).
The \textit{Backpropagation Algorithm}, an interative process similar to \gls{gd}, does exactly that: Evaluating the gradient of the loss function on the last layer and inversely forwarding the changes to the layer before it, and so on.
From there, inferences on the weights, biases and other parameters can be made in order to update them for the next iteration and start over, hopefully working ourselves towards the minimal loss possible \parencite{bishop-pattern-recognition-and-ml}.

As a final note to better understand the implications and possibilities of a large neural network, consider the following universal approximation theorem:

\begin{theorem}{Universal Approximation}{universal-approx}
  If the neural network has at least one hidden layer, proper nonlinear activation functions and enough data and hidden units, it can approximate any continuous function $y(x, w): \R^n \mapsto \R$ arbitrarily well on a compact domain \parencite{1989-HornikMultilayerFN}.
\end{theorem}

In the case of our demonstrator, the network consists of two fully connected layers with a Taylor-approximated \cryptop{relu} activation function in between.
For more details on the implemented neural networks' structure, code and performance, refer to \cref{chap:implementation} and \cref{chap:results}.

An alternative approach to modelling higher dimensional data would be \textit{Gaussian Processes}, a \glsdesc{ml} technique with a focus on different applications than neural networks', but just as powerful \parencite[Chapter 45]{2005-mackay-information-theory}.
The mathematical structure behind the model is a multivariate Gaussian distribution $$p: \R^n \mapsto \R,\, p(\vec{x}) = \frac{1}{\sqrt{(2\pi)^n |\Sigma|}} e^{-\frac{1}{2} (\vec{x}-\vec{\mu})^T \Sigma\inv (\vec{x}-\vec{\mu})} \,,$$ in many cases allowing for explicit analytical expressions and calculations as compared to multi-layered neural networks.

\pagebreak
\section{Post-Quantum Security}
\label{sec:post-quantum-sec}
\begin{figure}[H]
  \centering
  \inputtikz{figures/wave-function}
  \caption[Illustration of a wave function]{Illustration of a wave function $\tilde{\psi}: \R^2 \mapsto \R$ as commonly used in quantum mechanics.}
  \label{fig:wave-function}
\end{figure}

In quantum mechanics, we seek a mathematical description of quantum phenomena, commonly building upon \name{Schrödinger}'s formalisms based on wave functions and the basic postulates of quantum mechanics.

The mathematical foundation of quantum mechanics is deeply rooted in linear algebra and functional analysis.
An important concept is that of function spaces and, especially, Hilbert spaces.
Function spaces are a widely useful concept, polynomial rings are a great example too (confer \cref{corollary:bounded-polynomials-mod-q}).
Wave functions $\psi: \C^3 \mapsto \R$ are usually chosen as elements of the $\mathcal{L}^2$-space, the space of square-integrable\footnote{Mathematicians usually formalise these using \name{Lebesgue}-integrals instead of the commonly used \name{Riemann} formulation of an integral. \name{Lebesgue} integration allows for a much broader class of integrable functions and is usually the preferred method in this context.} functions:
$$\mathcal{L}^2 = \big\{\psi: \C^3 \mapsto \C \,\big|\, ||\psi|| < \infty\big\} \quad \text{with} \; ||\psi|| = \int_{-\infty}^{\infty} \psi^*(\vec{x}) \psi(\vec{x}) \,d^3x$$
with $||\psi||$ referred to as the $l_2$-norm of the function $\psi$.
By far not all functions are square integrable though, first and foremost, polynomials do not decrease in their absolute value towards $-\infty$ and $\infty$ leading to $||\psi|| \rightarrow \infty$, they are clearly not square integrable.
An example that does work would be a normal distribution, or any function of the \name{Schwartz} space.

The traditional Copenhagen interpretation relates a wave function to the probability that a particle is at the current position $\vec{r}$ at time $t$ at the given time.
Namely, this probability is given by $|\psi(\vec{r}, t)|^2$.
The square-integrability requirement is imposed on the wave function $\psi$ in order to make it normalizable, i.e. ensure that the total probability of presence is finite (or exactly $1$) when integrated over all possible states the system might be in.

When describing a quantum particle or system, physicists usually work with mathematical objects in three different spaces:
\begin{itemize}
  \item Position space inhibited by the wave function $\psi(\vec{r}, t) = \braket{r | s_1, s_2, ..., s_n} \in \mathcal{L}^2$ (a function space),
  \item Momentum space given by the Fourier-transformed wave function $\overline{\psi}(\vec{p}, t) = \braket{p | s_1, s_2, ..., s_n}$ $\in \mathcal{L}^2$ (also a function space) and
  \item State space, encompassing all possible basis states in which a system might currently be, a description that is usually highly specific to the problem we aim to solve with it.
        In the discrete, finite-dimensional case, represented by $\ket{s_1, s_2, ..., s_n} \in \mathbb{S}$.
\end{itemize}

The electrons orbiting an atomic nucleus for instance, can be uniquely described by four quantum numbers (and corresponding Hermitian operators) forming a discrete state space: $n$, $l$, $m_l$ and $m_s$ with wave function $\braket{r | n, l, m_l, m_s}$ and momentum function $\braket{p | n, l, m_l, m_s}$.

Consider the following system of two base states $\ket{0}$ and $\ket{1}$, together forming an orthonormal basis.
Due to \textit{Quantum Superposition}, the measured system can be in any linear combination of the two,
$$\ket{Q} = \alpha \ket{0} + \beta \ket{1},\; \alpha, \beta \in \C\,,$$
while enforcing that the scalar product of $\bra{Q}$ with $\ket{Q}$ is normalized to $1$ by the second axiom of probability theory, i.e. $$\braket{Q | Q} = \big(\alpha^* \bra{0} + \beta^* \bra{1}\big)\big(\alpha \ket{0} + \beta \ket{1}\big) = |\alpha|^2 + |\beta|^2 \overset{!}{=} 1 \,.$$
Here we use that $\{\ket{0}, \ket{1}\}$ comprise an orthonormal basis and therefore $\braket{0|1} = 0$, $\braket{1|0} = 0$, $\braket{0|0} = 1$ and $\braket{1|1} = 1$.
This seemingly simple system is referred to as a \textit{Qubit}, the basic unit of quantum information theory, uniquely represented by $\alpha$ and $\beta$.

Said to be 'at the heart of the disparity between classical and quantum physics', \textit{Quantum Entanglement}, which Einstein once referred to as "spooky action at a distance", breaks the physical principle of locality but is yet a fundamental part of most quantum theories \parencite{1989-unspeakable-qm}.
This phenomenon describes the connection between two or more quantum particles related to each other through the mutual dependence of their quantum states $\ket{s_1, s_2, ..., s_n}$.
Phrased differently, their states cannot be described independently of the rest of the particle group.
For instance, two antisymmetrically entangled fermions never expose the same spin when measured simultaneously, even when they are far apart from each other - this has also been shown experimentally \parencite{2013-entanglement-proof}.

Quantum computers exploit the physical properties of quantum systems such as superposition and entanglement in order to speed up computations.
They facilitate up to exponential speedups when algorithms are available, compared to problems on traditional computers.
In conventional complexity theory, problems are filed into different complexity classes when analyzing their runtime and memory usage.
There exist
\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt]
  \item NL (Nondeterministic Logarithmic space)
  \item P (Polynomial time)
  \item NP (Nondeterministic Polynomial time)
  \item PSPACE (Polynomial space)
  \item EXPTIME (Exponential time)
  \item EXPSPACE (Exponential space)
\end{enumerate}
computational complexity classes, sorted by the amount of problems contained in them (NL $\subseteq$ P $\subseteq$ NP $\subseteq$ PSPACE $\subseteq$ EXPTIME $\subseteq$ EXPSPACE).
A particularly interesting open problem is whether P = NP, one of the millenium prize problems.
The \gls{np} class of problems is very useful in cryptography, it is especially important for the hardness assumptions of cryptographic schemes, problems should be at least as hard as the hardest problems in \gls{np}.

\begin{definition}{NP-Hardness}{np-hard}
  A problem is referred to as \textit{NP-hard} \gls{iff} it is at least as hard as the hardest problems in the complexity class \glstext{np} (nondeterministic polynomial time). Formally written,
  $$\mathrm{NP} := \bigcup_{k \in \N} \cryptop{NTIME}(n^k)$$
  the union of all decision problems with runtime bounded by $\mathcal{O}(n^k)$.
\end{definition}

One of the first major algorithms in this context is \name{Shor}'s algorithm, discovered before the first working quantum computer was built.

\subsection{Shor's Algorithm}
In the \gls{rsa} scheme, but also other cryptographic schemes such as the \name{Diffie}-\name{Hellman} key exchange, the whole scheme's security is based on the hardness assumption of the integer factorisation problem.
As of today, the cryptographic community still lacks a proof that the factorisation problem is in \gls{np}, yet it is widely believed to be.
What we do know however, is that \gls{rsa} encryptions can be broken and signatures forged, as soon as a sufficiently powerful quantum computer (with enough Qubits of memory) is built, in part explaining the global interest of governments and security organisations in the topic.

The algorithm enabling this was invented by Peter \name{Shor} in 1994 and will be outlined here shortly as it is a core element to security considerations of modern cryptosystems.
The core structure of the algorithm is
\begin{enumerate}
  \item randomly selecting a guess $g \in \N$ that we hope shares a factor with a large $N = p \cdot q$ ($p, q, N \in \N$),
  \item improving that guess by a quantum subroutine and
  \item applying \name{Euclid}'s algorithm to find $p$ and $q$ the factors of $N$.
\end{enumerate}
As soon as we found a guess $g$ that satisfies $\gcd(N, g) \neq 1$, the algorithm finishes and we are done \parencite{1997-shors-algorithm}.

The core factorisation idea is the following, not specific to quantum computation: We know that for a pair $g, N \in \N$, we can always find some $r \in \N$ such that
$$g^r = m N + 1,\, m \in \N\,,$$
we are looking for a $g^r$ that is exactly one more than a multiple of $N$.
Rearranging,
$$g^r - 1 = m N \;\Longleftrightarrow\; (g^\frac{r}{2} + 1)(g^\frac{r}{2} - 1) = m N$$
we have found two factors $g^\frac{r}{2} + 1$ and $g^\frac{r}{2} - 1$ (for even $r$) that share a common factor with $N$ and apply Euclid's algorithm to get $p$ and $q$.
These two factors $g^\frac{r}{2} + 1$, $g^\frac{r}{2} - 1$ might themselves be multiples of $N$, or $r$ might be odd, rendering the guess useless and we need to start over with a new $g \in \N$.
According to \cite{1997-shors-algorithm}, we find 'good' factors roughly \SI{35.7}{\percent} of the time with just one guess, with only ten tries we arrive at a success rate $> \SI{99}{\percent}$.
The plain guessing process can be done by any common computer, but is even more inefficient for large $N$ (and resulting large $r$) than just factoring $N$ using a general number field sieve (the state-of-the-art method of plain factorisation).

This is where the quantum component comes in, efficiently \textit{improving} the guess $g$, therefore finding $r$.
Using a quantum superposition, a quantum computer can calculate the output of a function for multiple superpositioned inputs \textit{simultaneously}, which is what makes them so incredibly fast in some applications.
When measuring the output state, the key idea of a quantum computation is to arrange the inputs in such a way that only useful output remains, while the other terms cancel each other out by destructive interference.

Thereby, we instruct the quantum computer to raise our guess $g$ by all possible powers $\in \N$ up to some boundary in order to obtain
$$\ket{1, g^1} + \ket{2, g^2} + \ket{3, g^3}, ...$$
which we then take modulo $N$, resulting in a superposition of remainders
$$\ket{1, [g^1]_N} + \ket{2, [g^2]_N} + \ket{3, [g^3]_N} + ... \,.$$

Here is where \name{Shor}'s key idea came in:
The remainders in the above superposition expose repetitions at a period of exactly $r$ (which, by our definition fulfills $g^r \equiv 1 \mod N$) because for any $a \in \Z$,
$$g^x \equiv g^{x + r} \equiv g^{x + 2r} \equiv ... \equiv g^{x + ar} \mod N$$
the remainders are periodic with frequency $\frac{1}{r}$.
The above can be quickly derived from $g^r = mN + 1$, therefore $$g^{x+r} = g^x g^r = (\tilde{m} N + [g^x]_N) (m N + 1) = (m \tilde{m} N + [g^x]_N m + \tilde{m}) N + [g^x]_N$$ is indeed congruent to $g^x \mod N$.

And therefore, we can use a \gls{qft}, discovered by Don \name{Coppersmith} in \citeyear{1994-qft}, on the superposition of remainders of powers of $g$ to find the period $r$ which is exactly what we were looking for.
From the output of
$$\cryptop{QFT}\big(\ket{1, [g^1]_N} + \ket{2, [g^2]_N} + \ket{3, [g^3]_N} + ...\big)$$
we obtain the dominant frequency $\frac{1}{r}$ yielding us our desired improved guess \parencite{1997-shors-algorithm}.
Which again, can be evaluated extremely quickly on the given superpositioned inputs using a quantum computer, at least in theory.
From here, we obtain $g^\frac{r}{2} + 1$ and $g^\frac{r}{2} - 1$ in order to finally find the factors $p$ and $q$ of $N$ using \name{Euclid}'s algorithm on $g^\frac{r}{2} \pm 1$ and $N$.

For large numbers $N$ however, today's quantum computers' performance is still far off from factoring $N$ into $p \cdot q$ effectively, for relatively small $N$ however, it is already possible.

\subsection{Outlook}
Another important advancement in quantum computing related to cryptography is Grover's algorithm which provides an asymptotic quadratic speedup for performing function inversion \parencite{1996-grovers-algorithm}.
It could be used to perform certain key recovery, collision or pre-image attacks on cryptographic schemes, essentially halving the bit security of many schemes (128 bit security would be reduced down to 64 bits), suggesting to double the involved key size bits in many encryption schemes in order to be safe against potential future quantum attacks.

Lattice-based cryptosystems are still safe against quantum computation, i.e. quantum computers have a negligible advantage compared to traditional computers when attempting to break lattice-based encryptions, performing key-recovery attacks, etc.
Two of these schemes will be presented in the next chapter, in \cref{sec:bfv} and \cref{sec:ckks}.
