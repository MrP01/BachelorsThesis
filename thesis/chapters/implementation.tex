\chapter{Implementation}
\label{chap:implementation}

\section{Chosen Software Architecture}
In the given setting, the most accessible frontend is commonly a JavaScript web application.
A web-based demonstrator to show how to classify handwritten digits when using \glslink{he}{homomorphic encryption} was implemented, comprised of a C++ server and a React\footnote{\url{https://reactjs.org/}} frontend, confer \cref{fig:frontend}.

To still make the classification run as quickly and efficiently as possible, a C++ binary runs in the backend providing an HTTP API to the frontend application.
In order to allow for more flexibility of the HTTP server, the initial approach was to pipe requests through a dedicated web application framework with database access that would allow, for instance, user management next to the basic classification.
However, the resulting communication and computation overhead, even when running with efficient protocols such as ZeroMQ, was too high.

Extending the accessibility argument to reproducibility, Docker is a solid choice \parencite{using-docker-in-science}.
The deployment is structured into two Docker images, \textit{classifier} and \textit{frontend}, easily scalable to multiple instance of the C++ upstream server using a round-robin load-balancing strategy of the single reverse proxy Nginx\footnote{\url{http://nginx.org/}}.

To run the attached demo project, simply execute
\begin{minted}{bash}
  docker-compose build
  docker-compose up
\end{minted}
in the \texttt{code} folder and point your browser to \url{https://localhost}.

Using a Docker Multi-Stage Build, the application images were optimized towards a zero-dependency Alpine Linux image which contains nothing but compiled binaries and linked libraries.
This is achieved by introducing intermediate layers including all necessary compiler libraries and dependencies and only copying the resulting binary to the final image.
Similarly, the \textit{frontend} build process of course requires Node JS, a common JavaScript engine used for the React compilation step, in a previous build layer.
Yet, the final image only serves static files without any further server logic required and thus omits the entire JavaScript engine and associated libraries for the image.
Details on the build process can be found in \texttt{code/classifier/classifier.Dockerfile} and \texttt{code/frontend/frontend.Dockerfile}.

\section{The MNIST dataset}
The \gls{mnist} dataset of handwritten digits \parencite{mnist-original} contains 60,000 train and 10,000 test images with corresponding labels.
Some sample images are displayed in \cref{fig:mnist}, with ascending labels from 0 to 9.
In order to stick to the traditional feedforward technique with data represented in vector format, therefore it is common to reshape data from $(28, 28)$ images (represented as grayscale values in a matrix) into a $784$ element vector.

\begin{figure}[H]
  \centering
  \inputtikz{figures/mnist}
  \caption[Sample images of the MNIST dataset]{
    Sample images of the MNIST dataset of handwritten digits \parencite{mnist-original}.
    The dataset contains 70,000 images of $28 \times 28$ grayscale pixels valued from 0 to 255 as well as associated labels (as required for supervised learning).
  }
  \label{fig:mnist}
\end{figure}

\gls{mnist} is one of the most commonly used datasets in \glsdesc{ml}, featured in an abundant number of tutorials and showcases, all in all it is fairly well studied.
The National Institute of Standards and Technology recently published an extended version with uppercase and lowercase letters of the Latin alphabet contained as well (EMNIST), processed in the same way as \gls{mnist}, which should also work with our demonstrator.

\section{Our Neural Network}
The neural network implemented in our demonstrator was trained using the unencrypted standard \gls{mnist} dataset of 60,000 images, split into \SI{90}{\percent} training and \SI{10}{\percent} validation data.
The training process itself was performed using the \textit{Tensorflow} machine learning framework in Python\footnote{\url{https://www.tensorflow.org/}}.
The implemented network then has the following layer structure (also confer \cref{fig:neural-network}):
\begin{align*}
  \text{Layer 1:}\; & \vec{h} = \cryptop{relu\_taylor}(M_1 \vec{x} + \vec{b_1}) \\
  \text{Layer 2:}\; & \vec{y} = \cryptop{softmax}(M_2 \vec{h} + \vec{b_2})
\end{align*}

Expressed in Python code, using the \textit{Keras} extension of Tensorflow,
\begin{minted}{python}
  import tensorflow as tf

  model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation=relu_taylor),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Activation(tf.keras.activations.softmax),
  ])
\end{minted}

For performance metrics and some statistical analysis of the network's accuracy, refer to \cref{sec:accuracy-precision-recall}.

To gain some intuition on what the two layers look like internally, the following plots of weights and biases have been made:
\begin{figure}[H]
  \centering
  \inputtikz{figures/layer-1-and-2}
  \caption[Weights and biases of our neural network]{
    First and Second Layer Weights and Biases of the trained neural network.
    The weight matrices ($784 \times 128$ and $128 \times 10$) are skewed to account for the different dimensions used.
    The biases have the same shape as the output of the matrix multiplication and the input of the next layer.
  }
  \label{fig:layer-1-and-2}
\end{figure}

After going through both layers (confer \cref{fig:layer-1-and-2}), with the approximative \cryptop{relu\_taylor} activation function in between, the second (last) layer of the network outputs a vector of 10 scalars, each corresponding to one digit from 0 to 9.
When finally applying the \cryptop{softmax} function to the output, we can interpret the values as probabilities for each digit.

\pagebreak
\section{Matrix-Vector Multiplication}
\label{sec:matmul}
The dot product that is required as part of the neural network evaluation process needs to be implemented on \gls{seal} ciphertexts as well.

There are multiple methods to achieve a syntactically correct dot product (matrix-vector multiplication) as described by \textcite{2018-gazelle} for (square) matrices.

\begin{enumerate}
  \item \textbf{Naïve MatMul} - simple to derive but impractical in practice due to the limited further applicability of the result consisting of multiple ciphertexts. Applicable to arbitrary matrix dimensions, i.e. matrices $M \in \R^{s \times t}$, of course limited by the unreasonably high memory consumption and computation time of this approach.
  \item \textbf{Diagonal MatMul} - a simple and practical solution applicable to square matrices $M \in \R^{t \times t}$ that has a major advantage compared to the previous method as the computation yields a single ciphertext object instead of many which can be directly passed on to a following evaluation operation.
  \item \textbf{Hybrid MatMul} - essentially extending the diagonal method by generalising the definition of the diagonal extraction mechanism to 'wrap around' in order to match the dimensionality of the input vector.
        Applicable to arbitrary matrix dimensions, i.e. matrices $M \in \R^{s \times t}$ and favourable compared to the Naïve Method.
  \item \textbf{Babystep-Giantstep MatMul} - a more sophisticated technique aiming to significantly reduce the number of Galois rotations as they are rather expensive to carry out, with a performance boost especially noticeable for higher matrix dimensions.
        Without further modification, applicable to square matrices.
\end{enumerate}

For the following, define
\newcommand{\rot}{\cryptop{rot}}
\newcommand{\diag}{\cryptop{diag}}
\begin{align*}
  \rot_j: \R^t \mapsto \R^t,             & \; \{\rot_j(\vec{x})\}_i = x_{i + j} \\
  \diag_j: \R^{t \times t} \mapsto \R^t, & \; \{\diag_j(M)\}_i = M_{i, (i+j)}
\end{align*}
with all indices $i, j \in \Z_t$ member of the cyclic quotient group $\Z_t := \Z / t \Z$ of all integers modulo $t$, meaning that overflowing indices simply wrap around again starting at index $0$ to simplify notation.
For the sake of compactness, we stick to this notation for the rest of the section.

\pagebreak
\subsection{The Naïve Method}
\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/matmul-naive}
  \caption[Naïve matrix multiplication method]{The naïve method to multiply a square matrix with a vector (adapted from \cite{2018-gazelle}).}
  \label{fig:naive-method}
\end{figure}
Term by term, one can express a matrix-vector product of $M \in \R^{s \times t}$ and
$\vec{x} \in \R^t$ as follows (also see \cref{fig:naive-method}):
$$\{M \vec{x}\}_i = \sum_{j=1}^{t} M_{ij} x_j \,.$$

Accordingly, a natural (or rather, naïve) way to model this multiplication in Microsoft \gls{seal} would be to
\begin{enumerate}
  \item encode each $i$-th matrix row $(M_{i,1}, M_{i,2}, ..., M_{i,t})$ using the \texttt{Encoder} with matching parameters to the ciphertext of the encoded vector $\vec{x}$.
  \item multiply each encoded row with the encrypted vector using \texttt{Evaluator.multiply\_plain()} to obtain the ciphertext vector $\vec{y_i} \in \R^t$ for row $i$.
  \item perform the 'rotate-and-sum' algorithm \parencite{2018-gazelle} on each resulting vector (ciphertext) $\vec{y_i}$ to obtain the actual dot product of the matrix row with the vector $\vec{x}$:
        \begin{enumerate}
          \item using Galois automorphisms, rotate the entries of $\vec{y_i}$ by $\frac{t}{2}$ elements to obtain $\rot_{\frac{t}{2}}(\vec{y_i})$.
          \item perform an element-wise sum $\vec{y_i} + \rot_{\frac{t}{2}}(\vec{y_i})$ whose first (and also second) half now contains the sum of the two halves of $\vec{y_i}$.
          \item repeat the previous two steps $\log_2(t)$ times, halving the split parameter $t$ each time until one obtains $1$ element, which yields us the requested sum of all entries $\sum_{k=1}^t \{\vec{y_i}\}_k$ as the dot product of $\vec{x}$ and $\vec{y_i}$.
        \end{enumerate}
  \item Given all the 'scalar' results of each row-vector dot product, we can construct the resulting matrix-vector product.
\end{enumerate}

\paragraph{Adapting to non-square matrices}
\label{subsec:non-square-matrices}
The weight matrices in the given classification setting are not at all square, on the contrary their output dimension tends to be much lower than the input dimension as the overall goal is to reduce it from $28^2 = 784$ down to $10$.

However, that also means one cannot directly apply the na\"ive or diagonal methods for multiplication.
This 'flaw' can be mitigated by a simple zero-padding approach in order to make the matrix square, filling in zeroes until the lower-sized dimension reaches the higher one.

\subsection{The Diagonal Method}
\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/matmul-diagonal}
  \caption[Diagonal matrix multiplication method]{The diagonal method to multiply a square matrix with a vector (adapted from \cite{2018-gazelle}).}
  \label{fig:diagonal-method}
\end{figure}

As can be seen in \cref{fig:diagonal-method}, we perform the vector-vector products over the diagonals of the matrix instead of the rows and rotate $\vec{x}$ by one for each rotation.

\begin{theorem}{Diagonal Method}{diagonal-method}
  Given a matrix $M \in \R^{t \times t}$ and a vector $\vec{x} \in \R^t$,
  the dot product between the two can be expressed as
  $$M \vec{x} = \sum_{j=0}^{t-1} \diag_j(M) \cdot \rot_j(\vec{x}) \,.$$
\end{theorem}

\begin{proof}
  For all indices $i \in \Z/t\Z$,
  $$\bigg\{\sum_{j=0}^{t-1} \diag_j(M) \cdot \rot_j(\vec{x})\bigg\}_i
    = \sum_{j=0}^{t-1} M_{i,(i+j)} x_{i+j}
    \overset{[k=i+j]}{=} \sum_{k=i}^{t+i-1} M_{ik} x_k
    = \sum_{k=0}^{t-1} M_{ik} x_k
    = \{M \vec{x}\}_i \,.$$
\end{proof}

The key idea of this optimization is to exploit the \gls{simd} structure of the encryption schemes, in particular that of \gls{ckks}, and aggregating the result in one of the ciphertext objects.

\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/rotation-error}
  \caption[Error development after rotations of the diagonal method]{Diagonal Method error development after each rotation of the input vector.}
  \label{fig:rotation-error}
\end{figure}

One major problem of the diagonal method is that it requires many Galois rotations of $\vec{x}$, which is slow and also causes a large error after too many consecutive rotations (confer \cref{fig:rotation-error}).

\subsection{The Hybrid Method}
\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/matmul-hybrid}
  \caption[Hybrid matrix multiplication method]{The hybrid method to multiply an arbitrarily sized matrix with a vector (adapted from \cite{2018-gazelle}).}
  \label{fig:hybrid-method}
\end{figure}
To further extend the previous matrix multiplication method to solve the problem (cf. \cref{subsec:non-square-matrices}), it is first necessary to extend the definition of the $\diag$ operator to non-square matrices $M \in \R^{s \times t}$.
For the following, extending the above definition:
$$\diag_j: \R^{s \times t} \mapsto \R^t, \; \{\diag_j(M)\}_i = M_{i, (i+j)} \,.$$

\begin{theorem}{Hybrid Method}{hybrid-method}
  For a matrix $M \in \R^{s \times t}$ with $t$ a whole multiple of $s$ and a vector $\vec{x} \in \R^t$,
  $$M \vec{x} = (y_i)_{i \in \Z/s\Z} \;\text{with}\; \vec{y} = \sum_{k=1}^{t / s} \rot_{ks}\bigg(\sum_{j=1}^s \diag_j(M) \cdot \rot_j(\vec{x})\bigg) \,.$$
\end{theorem}

\begin{proof}
  For all indices $i \in \Z/s\Z$,
  $$\{\vec{y}\}_i = \bigg\{\sum_{k=1}^{t / s} \rot_{ks}\bigg(\sum_{j=1}^s \diag_j(M) \cdot \rot_j(\vec{x})\bigg)\bigg\}_i = \sum_{k=1}^{t / s} \sum_{j=1}^{s} M_{i,(i+j) + ks} x_{(i+j) + ks} \,,$$
  substituting $l = i+j+ks$ and condensing the nested sums into one single summation expression since $\sum_{k=1}^{t / s} \sum_{j=1}^{s} f(j+ks) = \sum_{l=1}^{t} f(l)$, we obtain
  $$y_i = \sum_{l=1+i}^{t+i} M_{il} x_l = \sum_{l=1}^{t} M_{il} x_l = \{M \vec{x}\}_i \,.$$
  For a longer example on the index notation and derivation of the terms above including the condensed sum, refer to \cref{proof:bsgs-matmul}.
\end{proof}

Due to its low number of operations, the hybrid method almost always outperforms the na\"ive and diagonal methods \parencite{2018-gazelle}.
A schematic representation illustrating the process can be seen in \cref{fig:hybrid-method}.

To exemplarily discuss the implementation of an \gls{he} algorithm, we break down the following piece of code responsible for the hybrid matrix multiplication.
Thanks to \gls{seal}'s object-oriented interface and explicit, consistent naming, the code snippets are mostly self-explanatory in combination with some additional comments.

We first encode the matrix diagonals using the \texttt{seal::CKKSEncoder} into a vector of plaintexts (each containing the information of one diagonal).
\begin{minted}{cpp}
  // diagonal method preparation
  size_t in_dim = matrix.shape(0), out_dim = matrix.shape(1);
  std::vector<seal::Plaintext> diagonals = encodeMatrixDiagonals(matrix, encoder, evaluator, in_out.parms_id(), in_out.scale(), nullptr, OUT_DIM);
\end{minted}

Once this is done, we evaluate the inner sum $\sum_{j=1}^s \diag_j(M) \cdot \rot_j(\vec{x})$ using a loop.
\begin{minted}{cpp}
  // perform the actual multiplication
  seal::Ciphertext sum = in_out;  // makes a copy
  evaluator.multiply_plain_inplace(sum, diagonals[0]);  // performs the first vector-vector product
  for (auto offset = 1ULL; offset < in_dim; offset++) {
    seal::Ciphertext tmp;  // for all remaining offsets:
    evaluator.rotate_vector_inplace(in_out, 1, galois_keys);
    evaluator.multiply_plain(in_out, diagonals[offset], tmp);
    evaluator.add_inplace(sum, tmp);
  }
  in_out = sum;  // and we arrive at the first result
  evaluator.rescale_to_next_inplace(in_out);  // scale down once
\end{minted}

Finally, we exploit the repetitions in the resulting sum from above and rotate $t / s$ times to end up with the sum of all chunks, encoded in one final ciphertext vector.
\begin{minted}{cpp}
  // perform the rotate-and-sum algorithm
  seal::Ciphertext rotated = in_out; // makes a copy
  for (size_t chunk = 0; chunk < in_dim / out_dim; chunk++) {
    evaluator.rotate_vector_inplace(rotated, out_dim, galois_keys);
    evaluator.add_inplace(in_out, rotated);  // adds rotated result to itself
  }
  return in_out;
\end{minted}

In the \glsdesc{ml} context, it is common to have a much higher input dimension than output dimension, usually leading to fully connected layers with matrices $M \in \R^{s \times t}$ with $s \ll t$.
The hybrid method is especially efficient in these cases as it only requires $s + \frac{t}{s} = \frac{s^2 + t}{s}$ rotations and $s$ multiplications.

As outlined in \cref{sec:performance-benchmarks}, the hybrid method will prove to be the fastest method for the matrix sizes in our given project, with the 2\textsuperscript{nd} best numerical accuracy.

\subsection{The Babystep-Giantstep Method}
Since Galois rotations are the most computationally intensive operations in most cryptographic schemes used today \parencite{2021-pasta}, they take a large toll on the efficiency.
In order to reduce the number of rotations required, one can make use of the \gls{bsgs} optimisation as described in \cite{2018-faster-helib}, which works as follows:

\begin{theorem}{Babystep-Giantstep Method}{bsgs}
  Given a matrix $M \in \R^{t \times t}$ and a vector $\vec{x} \in \R^t$, with $t = t_1 \cdot t_2$ split into two \gls{bsgs} parameters $t_1, t_2 \in \N$ and
  $$\diag'_p(M) = \rot_{-\lfloor p/t_1 \rfloor \cdot t_1}(\diag_p(M))\,,$$
  one can express a matrix-vector multiplication as follows:
  \begin{equation*}
    M \vec{x} = \sum_{k=0}^{t_2-1} \rot_{(kt_1)} \bigg(
    \sum_{j=0}^{t_1-1} \diag'_{(kt_1+j)}(M) \cdot \rot_j(\vec{x})
    \bigg)
  \end{equation*}
  where $\cdot$ denotes an element-wise multiplication of two vectors.
\end{theorem}

A proof of the above theorem can be found in \cref{proof:bsgs-matmul}.

Note that the optimized matrix-vector multiplication only requires $t_1 + t_2$ rotations as we can store the $t_1$ inner rotations of the vector $\vec{x}$ for the upcoming evaluations.
For larger matrices and vectors (larger $t$), $t_1 + t_2$ are indeed much smaller than the conventional number of required rotations $t = t_1 \cdot t_2$ in the diagonal method, or $\frac{s^2 + t}{s}$ rotations in the hybrid method, which was the point of this modification in the first place.
However, the number of multiplications and additions remains high - at least $t$ multiplications are required, whereas the hybrid method only requires $s$ multiplications (potentially much lower than $t$).

As we will see in \cref{sec:performance-benchmarks}, the \gls{bsgs} method also improves the performance of our classification process compared to the diagonal method, yet for the chosen parameters it does not reach the speed of the hybrid method.
the approach of splitting the sum into two parts improves its performance close to that of the hybrid method.

\section{Polynomial Evaluation}
As the encryption hides the true value, we cannot directly evaluate the \cryptop{relu} function since we have no way of telling whether it is larger than $0$ or not.
Using a finite series we can approximate the function though, also in the encrypted domain, because we can leverage the addition and multiplication operations of \gls{ckks}.

Finding a such approximation is not particularly hard, \name{Taylor}'s theorem provides us with an explicit method to approximate a given analytic function in this way:
$$T_N f(x, x_0) := \sum_{n=0}^N \frac{f^{(n)}(x_0)}{n!} (x-x_0)^n$$
To accomplish the best possible result however, a \name{Chebyshev} interpolation targeted at the corresponding domain subset may be a more suitable choice.
Performing such an interpolation on the interval $[-5, 10] \subset \R$ around 0 yields us the polynomial
$$\cryptop{relu\_taylor}(x) = -0.006137 x^3 + 0.090189 x^2 + 0.59579 x + 0.54738$$
which is plotted next to the function it is supposed to interpolate in \cref{fig:taylor-relu}.

\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/taylor-relu}
  \caption[Comparison of the Relu activation function vs. its Taylor expansion]{
    Comparison of the $\cryptop{relu}(x)$ activation function vs. its series approximation $\cryptop{relu\_taylor}(x)$, plotted on the given domain.
    Outside of it, the two functions strongly disagree.
  }
  \label{fig:taylor-relu}
\end{figure}

Next, we want to implement this polynomial evaluation using \gls{seal}.
From the implementation perspective, there are three properties to keep in mind when operating on a \gls{seal} ciphertext (\texttt{seal::Ciphertext x}):

\begin{enumerate}
  \item Scale (retrieved using \texttt{x.scale()}) \\
        Can be adjusted with: \texttt{evaluator.rescale\_inplace()}
  \item Encryption Parameters (retrieved using \texttt{x.parms\_id()}) \\
        Can be adjusted with: \texttt{evaluator.mod\_switch\_to\_inplace()}
  \item Ciphertext Size (retrieved using \texttt{x.size()}) \\
        Can be adjusted with: \texttt{evaluator.relinearize\_inplace()}
\end{enumerate}

For addition, the scales must be the same and they luckily will not change after the operation.
The more delicate operation is ciphertext multiplication.
Each time one multiplies two ciphertexts, the scales multiply (logarithmically, they add up, i.e. the common representations in bits are added together).
The chain index reduces by 1, which for an encoded ciphertext depends on the coeff moduli.
There must be enough bits remaining to perform the multiplication, namely $\log_2(scale)$ bits.

To illustrate the connections of these properties to one another, the following explanation of Wei Dai, one of the maintainers of \gls{seal}, on the "Scale out of bounds" error gives some good insight:

\begin{quote}
  Scale has nothing to do with noise. "Scale out of bounds" can appear even if noise is extremely low. Although repeated multiplication of a ciphertext by a plaintext will slowly increase the noise, it is not the reason why you see "scale out of bounds".

  The "Scale out of bounds" error specifically means that the scale of a ciphertext or plaintext is larger than the product of all elements in \texttt{coeff\_modulus}. If you perform multiplications without rescaling, you can quickly see this error.

  The more rescaling you perform, the less elements will be left in \texttt{coeff\_modulus}. Even if you managed to have the same scale in a ciphertext after every multiplication and rescale, eventually the \texttt{coeff\_modulus} can be too small to accommodate another multiplication \parencite{2020-wei-github-comment}.
\end{quote}

When evaluating a polynomial in SEAL, the above considerations need to be kept in mind when encoding the coefficients, multiplying the ciphertext $\vec{x}$ with itself to reach higher powers and adding together the weighted monomials.
We of course leverage the \gls{simd} structure of the encryption scheme in order to turn the computation into an element-wise polynomial evaluation of the 1\textsuperscript{st} layer's output (an encrypted vector) before passing it on to the 2\textsuperscript{nd} layer.

\section{Further Implementation Challenges}
When performing arbitrary computations on a ciphertext in \gls{seal}, it is possible to encounter a \textit{transparent ciphertext} from time to time.
That is a ciphertext identically equal to 0 (mathematically, the tuple's second polynomial is zero, confer \cref{def:ckks-scheme}), and it is usually not considered a valid encryption as anyone who has it can immediately 'decrypt' it to its underlying value of 0.
However, for intermediate values of the computation at hand, forwading values from addition to multiplication multiple times in our network, this is not an issue as long as we do not send the value back to the client, i.e. expose its internal structure \parencite{kim-laine-on-transparent-ciphertexts}.

In our usual setting of a web-based demonstrator for recognising handwritten digits, this situation cannot be avoided - but as long as the client does not send a ciphertext identical to 0, our final classification result (a vector of 10 elements) will never be 0.
Therefore, the server binary is compiled with the \mintinline{bash}{-DSEAL_THROW_ON_TRANSPARENT_CIPHERTEXT=OFF} flag enabled, preventing a system halt in the case of an intermediary value evaluating to 0.

An important optimisation was to pre-encode the weight matrices and bias vectors upon startup of the server.
This halved the individual request-response times, especially for the fast \gls{bsgs} and hybrid methods.

The first layer multiplication is much slower than the second-layer one due to the large matrices, depending on the multiplication method, but around a factor of 60-100.
For more details on runtime benchmarks, refer to \cref{sec:performance-benchmarks}.
