\chapter{Implementation}
\label{chap:implementation}

\section{Chosen Software Architecture}
In the given setting, the most accessible frontend is commonly a JavaScript web application.
A web-based demonstrator to show how to classify handwritten digits when using \glslink{he}{homomorphic encryption} was implemented, comprised of a C++ server and a React\footnote{\url{https://reactjs.org/}} frontend, confer \cref{fig:frontend}.

To still make the classification run as quickly and efficiently as possible, a C++ binary runs in the backend providing an HTTP API to the frontend application.
In order to allow for more flexibility of the HTTP server, the initial approach was to pipe requests through a dedicated web application framework with database access that would allow, for instance, user management next to the basic classification.
However, the resulting communication and computation overhead, even when running with very efficient protocols such as ZeroMQ, was too high.

Extending the accessibility argument to reproducibility, Docker is a very solid choice \parencite{using-docker-in-science}.
The deployment is structured into two Docker images, \textit{classifier} and \textit{frontend}, easily scalable to multiple instance of the C++ upstream server using a round-robin load-balancing strategy of the single reverse proxy Nginx\footnote{\url{http://nginx.org/}}.

To run the attached demo project, simply execute
\begin{minted}{bash}
  docker-compose build
  docker-compose up
\end{minted}
in the \texttt{code} folder and point your browser to \url{https://localhost}.

Using a Docker Multi-Stage Build, the application images were optimized towards a zero-dependency Alpine Linux image which contains nothing but compiled binaries and linked libraries.
This is achieved by introducing intermediate layers including all necessary compiler libraries and dependencies and only copying the resulting binary to the final image.
Similarly, the \textit{frontend} build process of course requires Node JS, a common JavaScript engine used for the React compilation step, in a previous build layer, but only serves static files in the final image without any further server logic required.
Details on the build process can be found in \texttt{classifier.Dockerfile} and \texttt{frontend.Dockerfile}.

\section{The MNIST dataset}
The MNIST dataset of handwritten digits \parencite{mnist-original} contains 60,000 train and 10,000 test images with corresponding labels.
In order to stick to the traditional feedforward technique with data represented in vector format, therefore it is common to reshape data from $(28, 28)$ images (represented as grayscale values in a matrix)
into a $784$ element vector.

\begin{figure}[H]
  \centering
  \inputtikz{figures/mnist}
  \caption[Sample images of the MNIST dataset]{Sample images of the MNIST dataset of handwritten digits \parencite{mnist-original}. The dataset contains 70,000 images of 28x28 grayscale pixels valued from 0 to 255 as well as assoicated labels (as required for supervised learning).}
\end{figure}

\section{Our Neural Network}
The network implemented in our demonstrator, trained using the \textit{Tensorflow} machine learning framework in Python \footnote{\url{https://www.tensorflow.org/}}, has the following layer structure (also confer \cref{fig:neural-network}):
\begin{align*}
  \text{Layer 1:}\; & \vec{h} = \cryptop{relu\_taylor}(M_1 \vec{x} + \vec{b_1}) \\
  \text{Layer 2:}\; & \vec{y} = \cryptop{softmax}(M_2 \vec{h} + \vec{b_2})
\end{align*}

Expressed in Python code, using the \textit{Keras} extension of Tensorflow,
\begin{minted}{python}
import tensorflow as tf

model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation=relu_taylor),
  tf.keras.layers.Dense(10),
  tf.keras.layers.Activation(tf.keras.activations.softmax),
])
\end{minted}

For performance metrics and some statistical analysis of the network's accuracy, refer to \cref{sec:accuracy-precision-recall}.

\section{Matrix-Vector Multiplication}
\label{sec:matmul}
The dot product that is required as part of the neural network evaluation process needs to be implemented on SEAL ciphertexts as well.

There are multiple methods to achieve a syntactically correct dot product (matrix-vector multiplication) as described by \textcite{2018-gazelle} for (square) matrices.

\begin{enumerate}
  \item \textbf{Naïve MatMul} - very simple to derive but impractical in practice due to the limited further applicability of the result consisting of multiple ciphertexts. Applicable to arbitrary matrix dimensions, i.e. matrices $M \in \R^{s \times t}$, of course limited by the unreasonably high memory consumption and computation time of this approach.
  \item \textbf{Diagonal MatMul} - a simple and practical solution applicable to square matrices $M \in \R^{t \times t}$ that has a major advantage compared to the previous method as the computation yields a single ciphertext object instead of many which can be directly passed on to a following evaluation operation.
  \item \textbf{Hybrid MatMul} - essentially extending the diagonal method by generalising the definition of the diagonal extraction mechanism to 'wrap around' in order to match the dimensionality of the input vector.
        Applicable to arbitrary matrix dimensions, i.e. matrices $M \in \R^{s \times t}$ and favourable compared to the Naïve Method.
  \item \textbf{Babystep-Giantstep MatMul} - a more sophisticated technique aiming to significantly reduce the number of Galois rotations as they are rather expensive to carry out, with a performance boost especially noticeable for higher matrix dimensions.
        Without further modification, applicable to square matrices.
\end{enumerate}

For the following, define
\newcommand{\rot}{\cryptop{rot}}
\newcommand{\diag}{\cryptop{diag}}
\begin{align}
  \rot_j: \R^t \mapsto \R^t,             & \; \{\rot_j(\vec{x})\}_i = x_{i + j} \label{eq:rot-definition} \\
  \diag_j: \R^{t \times t} \mapsto \R^t, & \; \{\diag_j(M)\}_i = M_{i, (i+j)} \label{eq:diag-definition}
\end{align}
with all indices $i, j \in \Z_t$ member of the cyclic quotient group $\Z_t := \Z / t \Z$ of all integers modulo $t$, meaning that overflowing indices simply wrap around again starting at index $0$ to simplify notation.
For the sake of compactness, we stick to this notation for the rest of this section.

\pagebreak
\subsection{The Naïve Method}
\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/matmul-naive}
  \caption[Naïve matrix multiplication method]{The naïve method to multiply a square matrix with a vector (adapted from \cite{2018-gazelle}).}
\end{figure}

Term by term, one can express a matrix-vector product of $M \in \R^{s \times t}$ and
$\vec{x} \in \R^s$ as follows:
$$\{M \vec{x}\}_i = \sum_{j=1}^{t} M_{ij} x_j \,.$$

Accordingly, a natural (or rather, naïve) way to model this multiplication in \textit{Microsoft SEAL} would be to
\begin{enumerate}
  \item encode each $i$-th matrix row $(M_{i,1}, M_{i,2}, ..., M_{i,t})$ using the \cpp{Encoder} with matching parameters to the ciphertext of the encoded vector $\vec{x}$.
  \item multiply each encoded row with the encrypted vector using \cpp{Evaluator.multiply_plain()} to obtain the ciphertext vector $\vec{y_i} \in \R^s$ for row $i$.
  \item perform the 'rotate-and-sum' algorithm \parencite{2018-gazelle} on each resulting vector (ciphertext) $\vec{y_i}$ to obtain the actual dot product of the matrix row with the vector $\vec{x}$:
        \begin{enumerate}
          \item using Galois automorphisms, rotate the entries of $\vec{y_i}$ by $\frac{s}{2}$ elements to obtain $\rot_{\frac{s}{2}}(\vec{y_i})$.
          \item perform an element-wise sum $\vec{y_i} + \rot_{\frac{s}{2}}(\vec{y_i})$ whose first (and also second) half now contains the sum of the two halves of $\vec{y_i}$.
          \item repeat the previous two steps $\log_2(s)$ times, halving the split parameter $s$ each time until one obtains $1$ element, which yields us the requested sum of all entries $\sum_{k=1}^s \{\vec{y_i}\}_k$ as the dot product of $\vec{x}$ and $\vec{y_i}$.
        \end{enumerate}
  \item Given all the 'scalar' results of each row-vector dot product, we can construct the resulting matrix-vector product.
\end{enumerate}

\paragraph{Adapting to non-square matrices}
\label{subsec:non-square-matrices}
The weight matrices in the given classification setting are by no means square, on the contrary their output dimension tends to be much lower than the input dimension as the goal is to reduce it from $28^2 = 784$ to $10$ overall.

However, that also means one cannot directly apply the na\"ive or diagonal methods for multiplication.
This 'flaw' can be mitigated by a simple zero-padding approach in order to make the matrix square, filling in zeroes until the lower-sized dimension reaches the higher one.

\subsection{The Diagonal Method}
\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/matmul-diagonal}
  \caption[Diagonal matrix multiplication method]{The diagonal method to multiply a square matrix with a vector (adapted from \cite{2018-gazelle}).}
  \label{fig:diagonal-method}
\end{figure}

As can be seen in \cref{fig:diagonal-method}, we perform the vector-vector products over the diagonals of the matrix instead of the rows and rotate $\vec{x}$ by one for each rotation.

\begin{theorem}{Diagonal Method}{diagonal-method}
  Given a matrix $M \in \R^{t \times t}$ and a vector $\vec{x} \in \R^t$,
  the dot product between the two can be expressed as
  \begin{equation*}
    M \vec{x} = \sum_{j=0}^{t-1} \diag_j(M) \, \rot_j(\vec{x}) \,.
  \end{equation*}
\end{theorem}

The key idea of this optimization is to exploit the \gls{simd} structure of the encryption schemes, in particular that of \gls{ckks}, and aggregating the result in one of the ciphertext objects.

\begin{proof}
  For all indices $i \in \Z/t\Z$,
  $$\bigg\{\sum_{j=0}^{t-1} \diag_j(M) \, \rot_j(\vec{x})\bigg\}_i
    = \sum_{j=0}^{t-1} M_{i,(i+j)} x_{i+j}
    \overset{[k=i+j]}{=} \sum_{k=i}^{t+i-1} M_{ik} x_k
    = \sum_{k=0}^{t-1} M_{ik} x_k
    = \{M \vec{x}\}_i \,.$$
\end{proof}

\begin{figure}[H]
  \centering
  \pgfplotsset{/pgfplots/group/.cd,vertical sep=2.0cm}
  \inputtikz{figures/generated/rotation-error}
  \caption[Error development after rotations of the diagonal method]{Diagonal Method error development after each rotation of the input vector.}
  \label{fig:rotation-error}
\end{figure}

One major problem of the diagonal method is that it requires many Galois rotations of $\vec{x}$, which is slow and also causes a large error after too many consecutive rotations (confer \cref{fig:rotation-error}).

\subsection{The Hybrid Method}
\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/matmul-hybrid}
  \caption[Hybrid matrix multiplication method]{The hybrid method to multiply an arbitrarily sized matrix with a vector (adapted from \cite{2018-gazelle}).}
\end{figure}
To further extend the previous matrix multiplication method to solve the problem (cf. \cref{subsec:non-square-matrices}), it is first necessary to extend the definition of the $\diag$ operator to non-square matrices $M \in \R^{s \times t}$.
For the following, extending the above definition:
$$\diag_j: \R^{s \times t} \mapsto \R^t, \; \{\diag_j(M)\}_i = M_{i, (i+j)} \,.$$

\begin{theorem}{Hybrid Method}{hybrid-method}
  For a matrix $M \in \R^{s \times t}$ with $t$ a whole multiple of $s$ and a vector $\vec{x} \in \R^t$,
  $$M \vec{x} = (y_i)_{i \in \Z/s\Z} \;\text{with}\; \vec{y} = \sum_{k=1}^{t / s} \rot_{ks}\bigg(\sum_{j=1}^s \diag_j(M) \, \rot_j(\vec{x})\bigg) \,.$$
\end{theorem}

\begin{proof}
  For all indices $i \in \Z/s\Z$,
  $$\{\vec{y}\}_i = \bigg\{\sum_{k=1}^{t / s} \rot_{ks}\bigg(\sum_{j=1}^s \diag_j(M) \, \rot_j(\vec{x})\bigg)\bigg\}_i = \sum_{k=1}^{t / s} \sum_{j=1}^{s} M_{i,(i+j) + ks} x_{(i+j) + ks} \,,$$
  substituting $l = i+j+ks$ and condensing the nested sums into one single summation expression since $\sum_{k=1}^{t / s} \sum_{j=1}^{s} f(j+ks) = \sum_{l=1}^{t} f(l)$, we obtain
  $$y_i = \sum_{l=i}^{t+i-1} M_{il} x_l = \sum_{l=0}^{t-1} M_{il} x_l = \{M \vec{x}\}_i \,.$$
\end{proof}

The hybrid method almost always outperforms the na\"ive and diagonal methods \parencite{2018-gazelle}.

To exemplarily discuss the implementation of an \gls{he} algorithm, we break down the following piece of code responsible for the hybrid matrix multiplication.
Thanks to SEAL's object-oriented interface and explicit, consistent naming, the code snippets are mostly self-explanatory in combination with some additional comments.

We first encode the matrix diagonals using the \cpp{seal::CKKSEncoder} into a vector of plaintexts (each containing the information of one diagonal).
\begin{minted}{cpp}
// diagonal method preparation
size_t in_dim = matrix.shape(0), out_dim = matrix.shape(1);
std::vector<seal::Plaintext> diagonals = encodeMatrixDiagonals(matrix, encoder, evaluator, in_out.parms_id(), in_out.scale(), nullptr, OUT_DIM);
\end{minted}

Once this is done, we evaluate the inner sum $\sum_{j=1}^s \diag_j(M) \, \rot_j(\vec{x})$ using a loop.
\begin{minted}{cpp}
// perform the actual multiplication
seal::Ciphertext sum = in_out;  // makes a copy
evaluator.multiply_plain_inplace(sum, diagonals[0]);  // performs the first vector-vector product
for (auto offset = 1ULL; offset < in_dim; offset++) {
  seal::Ciphertext tmp;  // for all remaining offsets:
  evaluator.rotate_vector_inplace(in_out, 1, galois_keys);
  evaluator.multiply_plain(in_out, diagonals[offset], tmp);
  evaluator.add_inplace(sum, tmp);
}
in_out = sum;  // and we arrive at the first result
evaluator.rescale_to_next_inplace(in_out);  // scale down once
\end{minted}

Finally, we exploit the repetitions in the resulting sum from above and rotate $t / s$ times to end up with the sum of all chunks, encoded in one final ciphertext vector.
\begin{minted}{cpp}
// perform the rotate-and-sum algorithm
seal::Ciphertext rotated = in_out; // makes a copy
for (size_t chunk = 0; chunk < in_dim / out_dim; chunk++) {
  evaluator.rotate_vector_inplace(rotated, out_dim, galois_keys);
  evaluator.add_inplace(in_out, rotated);  // adds rotated result to itself
}
return in_out;
\end{minted}

In the \glsdesc{ml} context, it is common to have a much higher input dimension than output dimension, usually leading to fully connected layers with matrices $M \in \R^{s \times t}$ with $s \ll t$.
The diagonal method is especially efficient in these cases as it only requires $s + \frac{t}{s} = \frac{s^2 + t}{s}$ rotations and $s$ multiplications.

\subsection{The Babystep-Giantstep Method}
Since Galois rotations are the most computationally intensive operations in most cryptographic schemes used today \parencite{2021-pasta}, they take a large toll on the efficiency.
In order to reduce the number of rotations required, one can make use of the \gls{bsgs} optimisation as described in \cite{2018-faster-helib}, which works as follows:

\begin{theorem}{Babystep-Giantstep Method}{bsgs}
  Given a matrix $M \in \R^{t \times t}$ and a vector $\vec{x} \in \R^t$, with $t = t_1 \cdot t_2$ split into two \gls{bsgs} parameters $t_1, t_2 \in \N$ and
  $$\diag'_p(M) = \rot_{-\lfloor p/t_1 \rfloor \cdot t_1}(\diag_p(M))\,,$$
  one can express a matrix-vector multiplication as follows:
  \begin{equation*}
    M \vec{x} = \sum_{k=0}^{t_2-1} \rot_{(kt_1)} \bigg(
    \sum_{j=0}^{t_1-1} \diag'_{(kt_1+j)}(M) \cdot \rot_j(\vec{x})
    \bigg)
  \end{equation*}
  where $\cdot$ denotes an element-wise multiplication of two vectors.
\end{theorem}

A proof of the above theorem can be found in \cref{proof:bsgs-matmul}.

Note that the optimized matrix-vector multiplication only requires $t_1 + t_2$ rotations as we can store the $t_1$ inner rotations of the vector $\vec{x}$ for the upcoming evaluations.
For larger matrices and vectors (larger $t$), $t_1 + t_2$ are indeed much smaller than the conventional number of required rotations $t = t_1 \cdot t_2$ in the diagonal or hybrid method for instance, which was the point of this modification in the first place.

As we will see in \autoref{sec:performance-benchmarks}, the \gls{bsgs} method greatly improves the performance of our classification process.

\section{Polynomial Evaluation}
From the implementation perspective, there are three properties to watch out for when working with SEAL ciphertexts:

\begin{enumerate}
  \item Scale (retrieved using \cpp{x.scale()}) \\
        % \begin{quote}
        %   Scale has nothing to do with noise. "Scale out of bounds" can appear even if noise is extremely low. Although repeated multiplication of a ciphertext by a plaintext will slowly increase the noise, it is not the reason why you see "scale out of bounds".
        %   "Scale out of bounds" error specifically means that the scale of a ciphertext or plaintext is larger than the product of all elements in coeff\_modulus. If you perform multiplications without rescaling, you can quickly see this error. The more rescaling you perform, the less elements will be left in coeff\_modulus. Even if you managed to have the same scale in a ciphertext after every multiplication and rescale, eventually the coeff\_modulus can be too small to accommodate another multiplication.
        % \end{quote}
        % https://github.com/microsoft/SEAL/issues/182#issuecomment-646234787
        Can be adjusted with: \cpp{evaluator.rescale_inplace()}
  \item Encryption Parameters (retrieved using \cpp{x.parms_id()}) \\
        Can be adjusted with: \cpp{evaluator.mod_switch_to_inplace()}
  \item Ciphertext Size (retrieved using \cpp{x.size()}) \\
        Can be adjusted with: \cpp{evaluator.relinearize_inplace()}
\end{enumerate}

\todo{Explain the challenges of polyval}

\paragraph{Multiplication}
Each time one multiplies two ciphertexts, the scales multiply (logarithmically, they add up, i.e. the bits are added together).
The chain index reduces by 1. The chain index of an encoded ciphertext depends on the coeff moduli.
There must be enough bits remaining to perform the multiplication, namely $\log_2(scale)$ bits.

\paragraph{Addition}
The scales must be the same, but luckily they will not change.

% TODO:
% \section{Transparent Ciphertext}
% \begin{quote}
%   The problem is that you are subtracting a ciphertext from itself.
%   This kind of operation results in a ciphertext that is identically zero; this is called a transparent ciphertext. Such a transparent ciphertext is not considered to be valid because the ciphertext reveals its underlying plaintext to anyone who sees it, even if they don't have the secret key.
%   By default SEAL throws an exception when such a situation is encountered to protect you from a problem you may not have noticed.
%   If you truly know what you are doing and want to enable the creation of transparent ciphertexts, you can configure SEAL [...].
%   \parencite{kim-laine-on-transparent-ciphertexts}
% \end{quote}

% https://github.com/microsoft/SEAL/issues/276#issuecomment-777073477
% 'transparent ciphertexts (a.k.a. ciphertexts whose second polynomial is zero) are malformed and do not need the secret key to decrypt'

\section{Neural Network}
The neural network was trained using the unencrypted standard \gls{mnist} dataset of 50,000 images, split into \SI{90}{\percent} training and \SI{10}{\percent} validation data.

\begin{figure}[H]
  \centering
  \inputtikz{figures/generated/taylor-relu}
  \caption[Comparison of the Relu activation function vs. its Taylor expansion]{Comparison of the Relu activation function vs. its Taylor expansion.}
  \label{fig:taylor-relu}
\end{figure}
\todo{Describe Taylor approximation of Relu function a bit}

To gain some intuition on what the two layers look like internally, the following plots of weights and biases have been made:
\begin{figure}[H]
  \centering
  \inputtikz{figures/layer-1-and-2}
  \caption[Weights and biases of our neural network]{First and Second Layer Weights and Biases.}
  \label{fig:layer-1-and-2}
\end{figure}

\todo{Beschreibung der obigen Figures, gehen beide Plots auf eine Seite?}
